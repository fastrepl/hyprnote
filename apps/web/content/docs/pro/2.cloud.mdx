---
title: "Cloud Services"
section: "Pro"
description: "Managed cloud services for Pro users"
---

<Tip>
  If you've [subscribed to the Pro plan](/pricing) or started a free trial, you automatically get access to these services.
</Tip>

## Included Services

Pro users get access to managed cloud services that work out of the box:

| Service | Description | Status |
|---------|-------------|--------|
| `/chat/completions` | LLM endpoint for AI features (summaries, notes) | Available |
| `/mcp` | MCP server with web search and URL reading tools | Available |

Pro includes curated AI models that work out of the box. Your requests are proxied through our servers with automatic API key management. If you want to use a specific LLM provider, you can bring your own API key (BYOK) in Settings > Intelligence.

## MCP Tools

The MCP server provides two built-in tools:

**exa-search** - Search the web via Exa and get page text and highlights in results. Useful for researching topics mentioned in your meetings.

**read-url** - Visit any URL and return the content as markdown. Great for pulling in context from links shared during meetings.

## Why Use Cloud Services?

While Char aims to be fully transparent and controllable, cloud services help in two ways:

1. **Faster time-to-value** - Start using AI features immediately without configuring API keys or running local models.
2. **Managed complexity** - Get the benefits of multiple AI providers without managing each one yourself.

## Privacy & Security

The cloud server (`pro.hyprnote.com`) is [open-source](https://github.com/fastrepl/char/tree/main/apps/pro) and deployed in our Kubernetes cluster on AWS via GitHub Actions.

**Data handling:**
- Nothing is stored by us
- Only required data is sent to third-party services
- Current providers: Curated LLM models, Exa (web search), Jina AI (URL reading)

All requests are rate-limited and authenticated using your Pro subscription.

If you prefer to run AI locally instead, see [Local LLM Setup](/docs/faq/local-llm-setup) for LLMs and [Local Models](/docs/developers/local-models) for speech-to-text.
