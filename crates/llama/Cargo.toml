[package]
name = "llama"
version = "0.1.0"
edition = "2021"

# https://github.com/utilityai/llama-cpp-rs/blob/update-llama-cpp-2025-05-28/llama-cpp-2/Cargo.toml
[features]
default = []
metal = ["llama-cpp-2/metal"]
cuda = ["llama-cpp-2/cuda"]
vulkan = ["llama-cpp-2/vulkan"]
native = ["llama-cpp-2/native"]
openmp = ["llama-cpp-2/openmp"]

[dependencies]
hypr-gguf = { workspace = true }

encoding_rs = "0.8.35"
llama-cpp-2 = { git = "https://github.com/yujonglee/llama-cpp-rs", default-features = false, branch = "update-llama-cpp-2025-06-25" }
llama-cpp-sys-2 = { git = "https://github.com/yujonglee/llama-cpp-rs", default-features = false, branch = "update-llama-cpp-2025-06-25" }

async-openai = { workspace = true }
futures-util = { workspace = true }
tokio = { workspace = true, features = ["rt-multi-thread", "sync"] }
tokio-stream = { workspace = true }
tracing = { workspace = true }

serde = { workspace = true }
thiserror = { workspace = true }

[dev-dependencies]
hypr-buffer = { workspace = true }
hypr-data = { workspace = true }
hypr-gbnf = { workspace = true }
hypr-listener-interface = { workspace = true }
hypr-template = { workspace = true }

dirs = { workspace = true }
rand = "0.9.0"
serde_json = { workspace = true }
