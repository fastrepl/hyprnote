pub(crate) mod feedback;

use axum::{Router, routing::post};

use crate::config::SupportConfig;
use crate::mcp::mcp_service;
use crate::state::AppState;

pub use feedback::{FeedbackRequest, FeedbackResponse};

pub async fn router(config: SupportConfig) -> Router {
    let llm_config = hypr_llm_proxy::LlmProxyConfig::new(&config.openrouter)
        .with_models_default(vec![
            "openai/gpt-oss-120b".into(),
            "moonshotai/kimi-k2-0905".into(),
        ])
        .with_models_tool_calling(vec![
            "anthropic/claude-haiku-4.5".into(),
            "moonshotai/kimi-k2-0905:exacto".into(),
        ]);
    let llm_router = hypr_llm_proxy::router(llm_config);

    let state = AppState::new(config).await;
    let mcp = mcp_service(state.clone());

    Router::new()
        .nest(
            "/feedback",
            Router::new().route("/submit", post(feedback::submit)),
        )
        .nest("/support", Router::new().nest_service("/mcp", mcp))
        .nest_service("/support/llm", llm_router)
        .with_state(state)
}
